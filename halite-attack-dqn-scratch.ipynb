{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T10:32:08.959071Z",
     "iopub.status.busy": "2020-08-31T10:32:08.921668Z",
     "iopub.status.idle": "2020-08-31T10:32:59.401124Z",
     "shell.execute_reply": "2020-08-31T10:32:59.402024Z"
    },
    "papermill": {
     "duration": 50.493453,
     "end_time": "2020-08-31T10:32:59.402331",
     "exception": false,
     "start_time": "2020-08-31T10:32:08.908878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, mem_size:443, rew:263, avg:0.594, eps:1.0, running_avg_rew:0.594\n",
      "ep:1, mem_size:913, rew:157, avg:0.334, eps:1.0, running_avg_rew:0.464\n",
      "ep:2, mem_size:1326, rew:482, avg:1.167, eps:1.0, running_avg_rew:0.698\n",
      "ep:3, mem_size:1767, rew:342, avg:0.776, eps:1.0, running_avg_rew:0.718\n",
      "ep:4, mem_size:2154, rew:465, avg:1.202, eps:0.99, running_avg_rew:0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SmallModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "from kaggle_environments.envs.halite.helpers import *\n",
    "import numpy as np\n",
    "import math\n",
    "from random import seed\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(profile=\"short\")\n",
    "\n",
    "NUM_AGENTS = 4\n",
    "BOARD_SIZE = 21\n",
    "TURNS = 400\n",
    "EPSILON = 1.0 \n",
    "EPSILON_DECAY = 0.998\n",
    "TRAINING_ITERATIONS = 4\n",
    "EPOCHS = 2\n",
    "REPLACE_TARGET_INTERVAL = 10\n",
    "LEARNING_RATE = 0.1\n",
    "REPLAY_CAPACITY = 200000\n",
    "WARM_START_SAMPLES = 32*20\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "PRINT_INTERVAL = 1\n",
    "running_avg_reward = []\n",
    "episode_rewards = []\n",
    "deposit_factor=5\n",
    "attack_factor=10\n",
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = args[0]\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "env = make('halite', configuration={\"randomSeed\": 5, \"episodeSteps\": TURNS, \"size\": BOARD_SIZE}, debug=True)\n",
    "_ = env.reset(num_agents=NUM_AGENTS)\n",
    "\n",
    "ACTIONS = [\n",
    "    ShipAction.NORTH,\n",
    "    ShipAction.EAST,\n",
    "    ShipAction.SOUTH,\n",
    "    ShipAction.WEST,\n",
    "    ShipAction.CONVERT,\n",
    "    None #Disable None for training attack\n",
    "]\n",
    "\n",
    "#helper func control_map\n",
    "def control_map(ships,shipyards):\n",
    "        ITERATIONS = 3\n",
    "\n",
    "        res = np.copy(ships)\n",
    "        for i in range(1,ITERATIONS+1):\n",
    "            res += np.roll(ships,i,axis=0) * 0.5**i\n",
    "            res += np.roll(ships,-i,axis=0) * 0.5**i\n",
    "        temp = res.copy()\n",
    "        for i in range(1,ITERATIONS+1):\n",
    "            res += np.roll(temp,i,axis=1) * 0.5**i\n",
    "            res += np.roll(temp,-i,axis=1) * 0.5**i\n",
    "        \n",
    "        return res + shipyards\n",
    "\n",
    "def world_feature(board):\n",
    "    size = board.configuration.size\n",
    "    me = board.current_player\n",
    "    \n",
    "    ships = np.zeros((1, size, size)) \n",
    "    ship_cargo = np.zeros((1, size, size))\n",
    "    shipyard = np.zeros((1, size, size)) \n",
    "\n",
    "    map_halite = np.array(board.observation['halite']).reshape(1, size, size)/1000\n",
    "\n",
    "    for iid, ship in board.ships.items():\n",
    "        ships[0, ship.position[1], ship.position[0]] = 1 if ship.player_id == me.id else -1\n",
    "        ship_cargo[0, ship.position[1], ship.position[0]] = ship.halite/1000\n",
    "\n",
    "    for iid, yard in board.shipyards.items():\n",
    "        shipyard[0, yard.position[1], yard.position[0]] = 1 if yard.player_id == me.id else -1\n",
    "\n",
    "    #Halite Spread\n",
    "    halite_spread = np.copy(map_halite)\n",
    "    for i in range(1,5):\n",
    "        halite_spread += np.roll(map_halite,i,axis=0) * 0.5**i\n",
    "        halite_spread += np.roll(map_halite,-i,axis=0) * 0.5**i\n",
    "    temp = halite_spread.copy()\n",
    "    for i in range(1,5):\n",
    "        halite_spread += np.roll(temp,i,axis=1) * 0.5**i\n",
    "        halite_spread += np.roll(temp,-i,axis=1) *  0.5**i\n",
    "    #Total Halite\n",
    "    halite_total = np.ones((1,size,size))*np.sum(ship_cargo)\n",
    "    #Mean Halite\n",
    "    halite_mean  = np.ones((1,size,size))*np.sum(ship_cargo)/(size**2)\n",
    "    \n",
    "    #Added Features Version 4:\n",
    "    #Player Numbers\n",
    "    player_num = len(board.players)\n",
    "    #Ship:\n",
    "    ship_map = np.zeros((player_num, size, size))\n",
    "    enemyships = []\n",
    "    for iid, ship in board.ships.items():\n",
    "        ship_map[ship.player_id][ship.position[1]][ship.position[0]] = 1\n",
    "        if ship.player_id != board.current_player_id:\n",
    "            enemyships.append(ship)\n",
    "\n",
    "    #Shipyard:\n",
    "    shipyard_map = np.zeros((player_num, size, size))\n",
    "    enemyShipyards = []\n",
    "    for iid, shipyard in board.shipyards.items():\n",
    "        shipyard_map[shipyard.player_id][shipyard.position[1]][shipyard.position[0]] = 1\n",
    "        if shipyard.player_id != board.current_player_id:\n",
    "            enemyShipyards.append(shipyard)\n",
    "    #Friendly units\n",
    "    ally = ship_map[board.current_player_id].reshape(1, size, size)\n",
    "    #Friendly shipyards\n",
    "    allyShipyard = shipyard_map[board.current_player_id].reshape(1, size, size)\n",
    "    \n",
    "    #Aggregate enemy units\n",
    "    all_enemy = np.sum(ship_map, axis=0) - ally\n",
    "    #Aggregate enemy shipyards\n",
    "    all_enemyShipyard = np.sum(shipyard_map, axis=0) - allyShipyard\n",
    "    #Basic Control Map \n",
    "    controlmap = control_map(ally-all_enemy,allyShipyard-all_enemyShipyard)\n",
    "    #Enemy ship halite\n",
    "    enemyShipHalite = np.zeros((1, size, size))\n",
    "    enemyShipHalite += np.Infinity\n",
    "    for iid, ship in board.ships.items():\n",
    "        if ship.player.id != board.current_player_id:\n",
    "            enemyShipHalite[0][ship.position[1]][ship.position[0]] = ship.halite/1000\n",
    "    \n",
    "#     global state\n",
    "#     np.set_printoptions(precision=3)\n",
    "#     state['configuration'] = board.configuration\n",
    "#     state['me'] = board.current_player_id\n",
    "#     state['playerNum'] = len(board.players)\n",
    "#     state['memory'] = {}\n",
    "#     state['currentHalite'] = board.current_player.halite\n",
    "#     state['next'] = np.zeros((board.configuration.size,board.configuration.size))\n",
    "#     state['board'] = board\n",
    "#     state['memory'][board.step] = {}\n",
    "#     state['memory'][board.step]['board'] = board\n",
    "#     state['cells'] = board.cells.values()\n",
    "#     state['ships'] = board.ships.values()\n",
    "#     state['myShips'] = board.current_player.ships\n",
    "#     state['shipyards'] = board.shipyards.values()\n",
    "#     state['myShipyards'] = board.current_player.shipyards\n",
    "#     N = state['configuration'].size\n",
    "\n",
    "#     # Halite \n",
    "#     state['haliteMap'] = np.zeros((N, N))\n",
    "#     for cell in state['cells']:\n",
    "#         state['haliteMap'][cell.position.x][cell.position.y] = cell.halite\n",
    "#     # Halite Spread\n",
    "#     state['haliteSpread'] = np.copy(state['haliteMap'])\n",
    "#     for i in range(1,5):\n",
    "#         state['haliteSpread'] += np.roll(state['haliteMap'],i,axis=0) * 0.5**i\n",
    "#         state['haliteSpread'] += np.roll(state['haliteMap'],-i,axis=0) * 0.5**i\n",
    "#     temp = state['haliteSpread'].copy()\n",
    "#     for i in range(1,5):\n",
    "#         state['haliteSpread'] += np.roll(temp,i,axis=1) * 0.5**i\n",
    "#         state['haliteSpread'] += np.roll(temp,-i,axis=1) *  0.5**i\n",
    "#     # Ships\n",
    "#     state['shipMap'] = np.zeros((state['playerNum'], N, N))\n",
    "#     state['enemyShips'] = []\n",
    "#     for ship in state['ships']:\n",
    "#         state['shipMap'][ship.player_id][ship.position.x][ship.position.y] = 1\n",
    "#         if ship.player_id != state['me']:\n",
    "#             state['enemyShips'].append(ship)\n",
    "#     # Shipyards\n",
    "#     state['shipyardMap'] = np.zeros((state['playerNum'], N, N))\n",
    "#     state['enemyShipyards'] = []\n",
    "#     for shipyard in state['shipyards']:\n",
    "#         state['shipyardMap'][shipyard.player_id][shipyard.position.x][shipyard.position.y] = 1\n",
    "#         if shipyard.player_id != state['me']:\n",
    "#             state['enemyShipyards'].append(shipyard)\n",
    "#     # Total Halite\n",
    "#     state['haliteTotal'] = np.sum(state['haliteMap'])\n",
    "#     # Mean Halite \n",
    "#     state['haliteMean'] = state['haliteTotal'] / (N**2)\n",
    "#     # Estimated \"value\" of a ship\n",
    "#     #totalShips = len(state['ships'])\n",
    "#     #state['shipValue'] = state['haliteTotal'] / state\n",
    "#     state['shipValue'] = ship_value()\n",
    "#     # Friendly units\n",
    "#     state['ally'] = state['shipMap'][state['me']]\n",
    "#     # Friendly shipyards\n",
    "#     state['allyShipyard'] = state['shipyardMap'][state['me']]\n",
    "#     # Enemy units\n",
    "#     state['enemy'] = np.sum(state['shipMap'], axis=0) - state['ally']\n",
    "#     # Enemy shipyards\n",
    "#     state['enemyShipyard'] = np.sum(state['shipyardMap'], axis=0) - state['allyShipyard']\n",
    "#     # Closest shipyard\n",
    "#     state['closestShipyard'] = closest_shipyard(state['myShipyards'])\n",
    "#     # Control map\n",
    "#     state['controlMap'] = control_map(state['ally']-state['enemy'],state['allyShipyard']-state['enemyShipyard'])\n",
    "#     state['negativeControlMap'] = control_map(-state['enemy'],-state['enemyShipyard'])\n",
    "#     state['positiveControlMap'] = control_map(state['ally'],state['allyShipyard'])\n",
    "#     #Enemy ship labeled by halite. If none, infinity\n",
    "#     state['enemyShipHalite'] = np.zeros((N, N))\n",
    "#     state['enemyShipHalite'] += np.Infinity\n",
    "#     for ship in state['ships']:\n",
    "#         if ship.player.id != state['me']:\n",
    "#             state['enemyShipHalite'][ship.position.x][ship.position.y] = ship.halite\n",
    "#     # Avoidance map (Places not to go for each ship)\n",
    "#     for ship in state['myShips']:\n",
    "#         state[ship] = {}\n",
    "#         state[ship]['blocked'] = get_avoidance(ship)\n",
    "#         state[ship]['danger'] = get_danger(ship.halite)\n",
    "#     # Who we should attack\n",
    "#     if len(state['board'].opponents) > 0:\n",
    "#         state['killTarget'] = get_target()\n",
    "#     for i in state:\n",
    "#         print(state[i])\n",
    "#     return 1\n",
    "    return np.concatenate([\n",
    "        map_halite, \n",
    "        ship_cargo, \n",
    "        halite_spread,\n",
    "        halite_total,\n",
    "        halite_mean,\n",
    "        shipyard_map,# these two maps are sepereted by player number (agent1234...) and channels should be +4\n",
    "        ship_map,\n",
    "        all_enemy, #note that all the enemy are aggregating together rather than spliting into different teams\n",
    "        all_enemyShipyard,\n",
    "        ally,\n",
    "        allyShipyard,\n",
    "        enemyShipHalite,\n",
    "        controlmap\n",
    "#         shipyard, #replace it with enemy/ally shipyard (shipyard_map)\n",
    "#         ships, #replace it with ship_map\n",
    "    ], axis=0)\n",
    "\n",
    "\n",
    "#As example take the first frame of the game\n",
    "sample_obs = env.state[0].observation\n",
    "board = Board(sample_obs, env.configuration)\n",
    "\n",
    "feature = world_feature(board)\n",
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#q-network\n",
    "class SmallModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_channels,\n",
    "                out_channels=16,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=1,\n",
    "                padding=3, #use this padding to give each pixel more vision\n",
    "                padding_mode='circular'\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=16,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=1,\n",
    "                padding=0, #this will make the padded first layer smaller again\n",
    "                padding_mode='circular'\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=16,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=1,\n",
    "                padding=0, #this will make the padded first layer smaller again\n",
    "                padding_mode='circular'\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(BOARD_SIZE*BOARD_SIZE*16, BOARD_SIZE*BOARD_SIZE*len(ACTIONS))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = self.network(features)\n",
    "        x = x.view(features.shape[0], -1) #flatten\n",
    "        x = self.output(x) #pass through linear layer\n",
    "        \n",
    "        return x.reshape(features.shape[0], self.num_actions, BOARD_SIZE, BOARD_SIZE)\n",
    "                          \n",
    "\n",
    "model = SmallModel(\n",
    "    input_channels=19, #needs to be equal to the number of feature channels we have\n",
    "    num_actions=len(ACTIONS)\n",
    ")\n",
    "\n",
    "target_model = SmallModel(\n",
    "    input_channels=19, #needs to be equal to the number of feature channels we have\n",
    "    num_actions=len(ACTIONS)\n",
    ")\n",
    "\n",
    "#predicting the feature from the cell above\n",
    "feature_tensor = torch.from_numpy(feature).float().unsqueeze(0)\n",
    "###\n",
    "def make_move(model, obs, configuration, EPSILON):\n",
    "    size = configuration.size\n",
    "    board = Board(obs, configuration)\n",
    "    me = board.current_player\n",
    "    #if we do not have ships but a shipyard build 1 ship\n",
    "    if len(me.ships)==0 and len(me.shipyards)>0:\n",
    "        me.shipyards[0].next_action = ShipyardAction.SPAWN\n",
    "    #Random Spawn, needs improvement\n",
    "    if len(me.ships)<=25:\n",
    "        for shipyard in me.shipyards:\n",
    "            i = np.random.randint(1,10)\n",
    "            if i<=3:\n",
    "                shipyard.next_action = ShipyardAction.SPAWN\n",
    "\n",
    "    #if we have no shipyard build one\n",
    "    state = world_feature(board).astype(np.float32)\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0)\n",
    "    \n",
    "    action_indices = model(state_tensor).detach().numpy().argmax(1).squeeze()\n",
    "    random_indices = np.random.choice(range(5), (size, size))\n",
    "    actions = np.zeros((size, size))-1 \n",
    "    \n",
    "\n",
    "    for ship in me.ships: \n",
    "        if len(me.shipyards)==0:\n",
    "            action_index = -1\n",
    "            ship.next_action = ShipAction.CONVERT #in our toy example we handle this manually\n",
    "        else:\n",
    "            if random.random() < EPSILON:\n",
    "                action_index = random_indices[ship.position[1], ship.position[0]]\n",
    "            else:\n",
    "                action_index = action_indices[ship.position[1], ship.position[0]]\n",
    "            \n",
    "            ship.next_action = ACTIONS[action_index]\n",
    "            \n",
    "        actions[ship.position[1], ship.position[0]] = action_index\n",
    "            \n",
    "    return me.next_actions, state, actions\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#print(model)\n",
    "memory = ReplayMemory(REPLAY_CAPACITY)\n",
    "\n",
    "for episode in range(TRAINING_ITERATIONS+1): #+1 so its inclusive and we print a statement at the end\n",
    "    print(f'{episode} - {round(EPSILON,3)} - {len(memory)}', end='\\r')\n",
    "    _ = env.reset(num_agents=NUM_AGENTS)\n",
    "\n",
    "    #When we call env.reset it will set the random seeds of both python and numpy to our fixed value\n",
    "    #We want to do some real random exploration though, otherwise we will always end up with the same game\n",
    "    seed_time = int(time.time()*1000)%1000000000\n",
    "    np.random.seed(seed_time)\n",
    "    seed(seed_time)\n",
    "    size = env.configuration.size\n",
    "\n",
    "\n",
    "    player2states = defaultdict(list)\n",
    "    player2actions = defaultdict(list)\n",
    "\n",
    "    player2halite = defaultdict(list)\n",
    "    player2rewards = defaultdict(list)\n",
    "\n",
    "    player2dones = defaultdict(list)\n",
    "\n",
    "    #The gist of this loop is copied from Tom Van de Wiele's answer here: https://www.kaggle.com/c/halite/discussion/144844\n",
    "    while not env.done:\n",
    "        observation = env.state[0].observation\n",
    "        player_mapped_actions = []\n",
    "        for active_id in range(NUM_AGENTS):\n",
    "            agent_status = env.state[active_id].status\n",
    "            if agent_status == 'ACTIVE':\n",
    "                player_obs = env.state[0].observation.players[active_id]\n",
    "                #print(len(board.current_player.ships))\n",
    "                c_b=0\n",
    "                for s in board.current_player.ships:\n",
    "                    c_b+=s.halite\n",
    "\n",
    "                #print(len(player_obs.ship_ids))\n",
    "                observation['player'] = active_id\n",
    "                prev_ships=len(board.current_player.ships)\n",
    "                #print('before action ships')\n",
    "                #print(player_obs)\n",
    "                engine_commands, state, actions = make_move(model, observation, env.configuration, EPSILON)\n",
    "                \n",
    "                    \n",
    "                \n",
    "                    \n",
    "                                \n",
    "                \n",
    "                after_ships=len(board.current_player.ships)\n",
    "                c_b1=0\n",
    "                for s in board.current_player.ships:\n",
    "                    c_b1+=s.halite\n",
    "         \n",
    "                        \n",
    "                \n",
    "                \n",
    "                #print('after action ships')\n",
    "                #print(player_obs)\n",
    "                #print(len(board.current_player.ships))\n",
    "                player2states[active_id].append(state)\n",
    "                player2actions[active_id].append(actions)\n",
    "\n",
    "                #in the first round there was no previous reward, we will use 5000 so the diff is 0 and drop it later in post processing\n",
    "                #one big thing is that we only generate one reward per frame, probably we should generate a (size, size) reward array to \n",
    "                #properly attribute the rewards to the ships if we have a multi ship scenario later\n",
    "                prev_reward = 5000 if len(player2halite[active_id]) == 0 else player2halite[active_id][-1]\n",
    "                \n",
    "                reward = player_obs[0] - prev_reward\n",
    "                reward=reward+(after_ships-prev_ships)\n",
    "                \n",
    "                if(c_b1<c_b):\n",
    "                    if(after_ships>=prev_ships):\n",
    "                        #some ships has deposited the cargo without any collison\n",
    "                        reward+=(c_b-c_b1)*deposit_factor\n",
    "                    elif(after_ships<prev_ships):\n",
    "                        #some ships got attacked #avoid collison\n",
    "                        reward-=(c_b-c_b1)\n",
    "                elif(c_b1>=c_b):\n",
    "                    if(after_ships>=prev_ships):\n",
    "                        #collected halite\n",
    "                        reward+=(c_b1-c_b)#no deposit\n",
    "                    elif(after_ships<prev_ships):\n",
    "                        #ships got attacked but also collected some halite\n",
    "                        reward-=(c_b1-c_b)+(after_ships-prev_ships)\n",
    "                in_danger=0\n",
    "                in_danger_total=0\n",
    "                for s in board.current_player.ships:\n",
    "                    in_danger=0\n",
    "                    sc=s.cell\n",
    "                    if sc.north.ship is not None:\n",
    "                        if sc.north.ship.player_id!=s.player_id:\n",
    "                            if sc.north.ship.halite<s.halite:\n",
    "                                in_danger+=1\n",
    "                    if sc.south.ship is not None:\n",
    "                        if sc.south.ship.player_id!=s.player_id:\n",
    "                            if sc.south.ship.halite<s.halite:\n",
    "                                in_danger+=1\n",
    "                    if sc.east.ship is not None:\n",
    "                        if sc.east.ship.player_id!=s.player_id:\n",
    "                            if sc.east.ship.halite<s.halite:\n",
    "                                in_danger+=1\n",
    "                    if sc.west.ship is not None:\n",
    "                        if sc.west.ship.player_id!=s.player_id:\n",
    "                            if sc.west.ship.halite<s.halite:\n",
    "                                in_danger+=1\n",
    "                    in_danger_total+=in_danger+s.halite\n",
    "                    \n",
    "                reward-=(in_danger_total*attack_factor)\n",
    "                \n",
    "                \n",
    "                reward = reward if reward > 0 else 0\n",
    "                player2rewards[active_id].append(reward) \n",
    "                player2halite[active_id].append(player_obs[0]) \n",
    "\n",
    "                player2dones[active_id].append(env.done)\n",
    "\n",
    "                player_mapped_actions.append(engine_commands)\n",
    "            else:\n",
    "                player_mapped_actions.append({})\n",
    "        env.step(player_mapped_actions)\n",
    "\n",
    "\n",
    "    #Postprocessing:\n",
    "    #We need to build (state(t), actions(t), reward(t+1), state(t+1), dones(t+1)) tuples\n",
    "    #After the env finished we want to set the last done to true\n",
    "    #We want to add the last reward and remove the reward t=0 (since we always need reward(t+1))\n",
    "    for active_id in range(NUM_AGENTS):\n",
    "        player_obs = env.state[0].observation.players[active_id]\n",
    "        \n",
    "        player2dones[active_id][-1] = True #the main loop does not get called again when the env is done so we set it manually\n",
    "\n",
    "        prev_reward = player2halite[active_id][-1]\n",
    "        reward = player_obs[0] - prev_reward\n",
    "        \n",
    "        reward = reward if reward > 0 else 0\n",
    "        player2rewards[active_id].append(reward) #append reward t+1\n",
    "        player2rewards[active_id].pop(0) #remove reward t=0\n",
    "\n",
    "        #For debugging: Make sure we have the same number of samples everywhere\n",
    "        #print(len(player2states[active_id]), len(player2actions[active_id]),len(player2rewards[active_id]),len(player2dones[active_id]),)\n",
    "        #Look at your rewards and compare with the replay below whether the reward matches the games that you see\n",
    "        #print(player2rewards[active_id])\n",
    "\n",
    "\n",
    "        states = player2states[active_id]\n",
    "        next_states = [x for x in states]\n",
    "        next_states = next_states[1:] + next_states[-1:]\n",
    "\n",
    "        for state, action, reward, next_state, done in zip(states, player2actions[active_id], player2rewards[active_id], next_states, player2dones[active_id]):\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    episode_rewards.append(np.array([x for y in player2rewards.values() for x in y]))\n",
    "\n",
    "        \n",
    "        \n",
    "    running_avg_reward.append(episode_rewards[-1].sum()/episode_rewards[-1].shape)\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        episode_rewards = np.concatenate(episode_rewards)\n",
    "        print(f'ep:{episode}, '\n",
    "               f'mem_size:{len(memory)}, '\n",
    "               f'rew:{episode_rewards.sum()}, '\n",
    "               f'avg:{round(episode_rewards.sum()/episode_rewards.shape[0], 3)}, '\n",
    "               f'eps:{round(EPSILON, 2)}, '\n",
    "               f'running_avg_rew:{round(np.mean(running_avg_reward), 3)}'\n",
    "              )\n",
    "        episode_rewards = []\n",
    "    \n",
    "            \n",
    "            \n",
    "    if not len(memory)>WARM_START_SAMPLES:\n",
    "        continue\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        sample = memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation). \n",
    "        # This converts [(state1, action1, reward1, next_state1, done1), (state2, action2, reward2, next_state2, done2)]\n",
    "        # To:[(state1, state2), (action1, action2), (reward1, reward2), (next_state1, next_state2), (done1, done2)]\n",
    "        states, actions, rewards, next_states, dones = list(zip(*sample))\n",
    "\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones)\n",
    "\n",
    "        #Its not a bad idea to check shapes again: states.shape, next_states.shape, actions.shape, rewards.shape, dones.shape\n",
    "        states = torch.from_numpy(states)\n",
    "        next_states = torch.from_numpy(next_states)\n",
    "        actions = torch.from_numpy(actions).long()\n",
    "        rewards = torch.from_numpy(rewards)\n",
    "        dones = torch.from_numpy(dones)\n",
    "\n",
    "        #our actions are a (size, size) array, we want to select all fields that are not -1 since this is where a ship was that took an action\n",
    "        #if we had mutliple ships then batch would contain the same frame multiple times with different x and y coordinates\n",
    "        batch, xs, ys = np.where(actions>-1)\n",
    "\n",
    "        taken_actions = actions[batch, xs, ys].unsqueeze(-1)\n",
    "\n",
    "        #We will train multiple epochs, here you would ideally want to sample from a replay buffer\n",
    "\n",
    "        current_qs = model(states)[batch, :, xs, ys].gather(1, taken_actions)\n",
    "        next_qs = target_model(next_states).detach().max(1)[0][batch, xs, ys]\n",
    "\n",
    "        # target_q = reward + 0.99 * next_state_max_q * (1 - done)\n",
    "        target_qs = rewards[batch] + GAMMA * next_qs * ~dones[batch]\n",
    "        loss = F.smooth_l1_loss(current_qs.squeeze(), target_qs.detach())\n",
    "        #if we turn this on we will see that the loss is actually not decreasing very much from epoch to epoch\n",
    "        #print(target_qs.shape, current_qs.shape, loss.mean(), end='\\n')\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "\n",
    "    if episode and episode % REPLACE_TARGET_INTERVAL:\n",
    "        target_model = copy.deepcopy(model)\n",
    "        \n",
    "    EPSILON *= EPSILON_DECAY \n",
    "\n",
    "torch.save(model, './model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011807,
     "end_time": "2020-08-31T10:32:59.427896",
     "exception": false,
     "start_time": "2020-08-31T10:32:59.416089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 55.836646,
   "end_time": "2020-08-31T10:32:59.545866",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-31T10:32:03.709220",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
